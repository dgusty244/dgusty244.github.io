---
layout: default
title: Drew Gustafson | Portfolio
---

<div class="container">
  <img src="images/LinkedinPfp.png" alt="My Profile Picture" class="profile-pic" />
  <h1 class="title">Drew Gustafson | Senior Data Engineer</h1>

  <a href="documents/DrewGustafsonOfficialResume.pdf" download class="btn" style="margin-top:10px;margin-bottom:30px;">
    ⬇ Download Resume (PDF)
  </a>

  <p><strong>Location:</strong> Seattle, WA<br>
     <strong>Email:</strong> drewgustafson12@gmail.com<br>
     <strong>Phone:</strong> (206) 849-2709<br>
     <a href="https://linkedin.com/in/dtg3" target="_blank" class="btn" rel="noopener noreferrer">linkedin.com/in/dtg3</a>
     <a href="https://github.com/dgusty244" target="_blank" class="btn" rel="noopener noreferrer">github.com/dgusty244</a>
  </p>

  <h3>Professional Summary</h3>
  <p>Senior Data Engineer with 4+ years of experience designing metadata-driven ETL systems and building data warehouses using Azure. Led the design of an ETL framework at WTW and delivered scalable data solutions at Insight. Skilled in integrating disparate data sources, optimizing pipeline performance, and developing robust systems that reduce development effort for new projects.</p>

  <h3>Work Experience</h3>

  <p><strong>Willis Towers Watson | Data Engineer | Remote | June 2023 – Present</strong></p>
<ul>
  <li>Designed and implemented a metadata-driven ETL framework in Azure, significantly reducing code redundancy and minimizing the number of ADF pipelines and Databricks notebooks. Supported multiple data sources including SFTP, Delta Sharing, and Event Hub. Documented framework and trained teammates on its usage.</li>
  <li>Designed the framework with scalability in mind, allowing future integration of sources such as high-volume OLTP databases on SQL Server, Salesforce, or Oracle with minimal code changes.</li>
  <li>Modernized scheduling architecture by implementing metadata-driven DAGs, enabling dependency-based orchestration and eliminating brittle fixed-order sequencing.</li>
  <li>Built a streaming-like ingestion mechanism that detects file arrivals from SFTP and dynamically orchestrates downstream workflows, improving SLAs and reducing latency in reporting pipelines.</li>
  <li>Optimized incremental loads by fixing boundary logic bugs and switching SFTP ingestion from fixed lookback periods to watermark-based incremental loads using ETL-derived timestamps.</li>
  <li>Increased pipeline resilience by developing Python UDFs to handle CSV irregularities, including embedded commas, line breaks, inconsistent delimiters, and schema drift.</li>
  <li>Designed and owned an automated monitoring tool using Python and Logic Apps to track pipeline health, identify silent failures, and surface stale data, improving observability and operational reliability.</li>
</ul>

<p style="margin-bottom: 0;"><strong>Insight Enterprises (Clients: Microsoft, Gilead, Microchip, TQL) | Remote | April 2021 – May 2023</strong></p>
<p style="margin-top: 0;"><strong>Career Progression: Associate Data Engineer - Data Engineer - Senior Data Engineer</strong></p>
<ul>
  <li>Ingested and transformed millions of rows across batch pipelines using ADF, Databricks, and SQL; supported high-scale data modeling in a medallion architecture for multiple client-facing dashboards.</li>
  <li>Designed and delivered a full-stack data platform, modeling denormalized flat files into a normalized, Kimball-style star-schema warehouse, and developed Power BI reports tailored to client needs.</li>
  <li>Engineered parameterized, metadata-driven PySpark notebooks to refresh hundreds of cloud tables.</li>
  <li>Led a Synapse migration project, implemented scheduling, job configurations, and incremental loads via ADF and stored procedures on the dedicated SQL pool.</li>
  <li>Deployed Azure infrastructure for a greenfield data platform with Terraform; leveraged and maintained existing CI/CD pipelines for deployment automation.</li>
  <li>Built data pipelines powering ML models for product recommendations and defect detection, collaborating with data scientists to deliver production-ready feature inputs and refresh logic.</li>
  <li>Built Python-based validation tools for row-level table diffs and duplicate key detection across environments.</li>
</ul>


  <p><strong>USI Insurance Services | Operations Data Analyst | Remote | March 2020 – April 2021</strong></p>
  <ul>
    <li>Automated reports and data-prep tasks using PowerQuery and Excel, sparking transition into data engineering.</li>
    <li>Saved 2+ workdays/month by automating a monthly sales analytics report with 7 disparate sources in PowerQuery which joined all tables and transformed data to the exact specification of business leaders.</li>
    <li>Developed customer engagement reporting through requirements planning with upper management. Took the product through several sprints/iterations, incorporating feedback from end users.</li>
    <li>Performed ad hoc data reporting using conditional and Boolean logic, aggregation, crosstabs, and slicers.</li>
    <li>Automated monthly data transformation tasks such as formatting, conditional statements, extracting characters from strings, increasing data granularity by unpivoting columns allowing end users to drill down.</li>
  </ul>

  <h3>Education</h3>
  <p><strong>University of Washington, Michael G. Foster School of Business | Seattle, WA | September 2016 – June 2021</strong></p>
  <p>M.S. Information Systems (GPA 3.87), B.A. Business Administration – Info Systems & Supply Chain (GPA 3.62)</p>

  <h3>Technical Skills</h3>
  <ul>
    <li>Languages: Python, PySpark, Pandas, SQL (Recursive CTE, MERGE, DAG, Window Functions), Terraform</li>
    <li>Azure: Data Factory, Databricks, Synapse, Blob Storage, Key Vault, DevOps, Event Hub, Logic Apps</li>
    <li>Other: Data Modeling, Lakehouse Architecture, Power BI, Tableau, ML Support, CI/CD, Git</li>
  </ul>
</div>
